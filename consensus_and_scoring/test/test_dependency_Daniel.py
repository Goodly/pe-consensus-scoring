import pandas as pd
import sys
import os
import pandas as pd

import test_utils
from filegen_utils import *
from IAA import *
from Dependency import *
from Weighting import *
import conftest


sys.path.append('../../')

# This test will produce an error, I can't figure out he solution right now, but the error message could already give enough information.
def test_dep_sample_fail(config):
    iaa_files_path = test_utils.make_test_directory(config, 'dep_sample_Daniel')
    out_path = test_utils.make_test_directory(config, 'out_dep_sample_Daniel')
    weight_out_folder = test_utils.make_test_directory(config, 'out_weighting_dep_iaa_test')

    # source_task_id generated by smashing keyboard
    iaa = IAA_task(out_folder=iaa_files_path, source_task_id='dependency_testing')
    iaa.add_row({"agreed_Answer": 1, "question_Number": 1, "namespace": 'Covid_Probability', "agreement_score":'L',
                 'highlighted_indices': test_utils.make_highlight_indices(10,30)})
    iaa.add_row({"agreed_Answer": 1, "question_Number": 2, "namespace": 'Covid_Probability', "agreement_score":'M',
                 'highlighted_indices': test_utils.make_highlight_indices(10,30)})
    iaa.add_row({"agreed_Answer": 3, "question_Number": 2, "namespace": 'Covid_Probability', "agreement_score":'U',
                 'highlighted_indices': test_utils.make_highlight_indices(10,30)})
    # iaa.add_row({"agreed_Answer": 3, "question_Number": 2, "namespace": 'Covid_Probability',  "agreement_score":'M'})
    iaa.add_row({"agreed_Answer": 2, "question_Number": 2, "namespace": 'Covid_Probability',  "agreement_score": 1})
    fin_path = iaa.export()

    
    data_path = config['data_dir']
    schema_path = data_path + '/schemas'
    dh_path = None #doesn't get used by dependency but is still an argument

    eval_dependency(dh_path, iaa_files_path, schema_path, out_path)

    for root, dir, files in os.walk(out_path):
        for file in files:
            #should be only 1 file for this case, so just run it on the only one
            # if there's more than 1 then you can get fancy
            out_df  = pd.read_csv(os.path.join(out_path, file), encoding='utf-8')

    weighting_out =  launch_Weighting(out_path, weight_out_folder)





def test_dep_sample_multiple_children(config):
    iaa_files_path = test_utils.make_test_directory(config, 'dep_sample')
    out_path = test_utils.make_test_directory(config, 'out_dep_sample')
    # source_task_id generated by smashing keyboard
    iaa = IAA_task(out_folder=iaa_files_path, source_task_id='kjncsa87nxao21899102j1j2')
    iaa.add_row({"agreed_Answer": 5, "question_Number": 2, "namespace": 'Covid_Evidence2020_03_21', "agreement_score":1})
    iaa.add_row({"agreed_Answer": 1, "question_Number": 7, "namespace": 'Covid_Evidence2020_03_21', "agreement_score":1})
    # iaa.add_row({"agreed_Answer": 1, "question_Number": 4, "namespace": 'Covid_Probability'})

    fin_path = iaa.export()
    data_path = config['data_dir']
    schema_path = data_path + '/schemas'
    dh_path = None #doesn't get used by dependency but is still an argument

    eval_dependency(dh_path, iaa_files_path, schema_path, out_path)
    out_df = None;
    for root, dir, files in os.walk(out_path):
        for file in files:
            #should be only 1 file for this case, so just run it on the only one
            # if there's more than 1 then you can get fancy
            out_df  = pd.read_csv(os.path.join(out_path, file), encoding='utf-8')
            print(out_df)
    assert(len(out_df) == 1)        





# def test_dep_iaa_failures(config, tmpdir):
#     out_path = test_utils.make_test_directory(config, 'testing_dep_iaa')
#     weight_out_folder = test_utils.make_test_directory(config, 'out_testing_dep_iaa')
#     iaa = IAA_task(out_folder=out_path, source_task_id='weightsampletests')
#     #-.5 points--from the weight key in config folder and the agreement_score
#     iaa.add_row({"namespace":"Covid_Evidence2020_03_21", "agreed_Answer": 1, "agreement_score":'L', "question_Number": 4})
#     #-2 points from ./config/weight_key and agreement score
#     iaa.add_row({"namespace": "Covid_Evidence2020_03_21", "agreed_Answer": 2,  "agreement_score":'U',"question_Number": 5})
#     iaa.add_row({"namespace": "Covid_Evidence2020_03_21", "agreed_Answer": 0,  "agreement_score":'M',"question_Number": 6})

#     # +1.5 points from ./config/weight_key and agreement score
#     # iaa.add_row({"namespace": "Covid_Evidence2020_03_21", "agreed_Answer": 'L', "question_Number": 12})
#     fin_path = iaa.export()

#     data_path = config['data_dir']
#     schema_path = data_path + '/schemas'
#     iaa_path = config['test_dir'] +'testing_dep_iaa'

#     eval_dependency(out_path, iaa_path, schema_path, config['test_dir'])
#     dep_path = config['test_dir'] + 'Dep_testing_dep_iaa/' +'iaa_weightsampletests.csv'
#     dep = pd.read_csv(dep_path, encoding='utf-8')
#     print(dep)
#     # if it fails the iaa tests due to the agreement_score, the shouldn't appear in the dep_iaa file
#     assert len(dep['prereq_passed']) == 0